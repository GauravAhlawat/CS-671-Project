{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_file = open('all_on_30th_nov.txt', \"r\")\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        tweets_data.append(tweet)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411703"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_ds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter(tweet_text):\n",
    "    if len(tweet_text) > 0:\n",
    "        tokens = tokenizer.tokenize(tweet_text)\n",
    "        tokens_filtered = []\n",
    "        for i in tokens:\n",
    "            if i.startswith('@') or i.startswith('#') or i.startswith('http'):\n",
    "                if i in stopWords:\n",
    "                    continue\n",
    "            else:\n",
    "                tokens_filtered.append(i.lower())\n",
    "        return tokens_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(tweets_data)):\n",
    "    try:\n",
    "        if tweets_data[i]['lang'] == 'en':\n",
    "            tweet_text = tweets_data[i]['text']\n",
    "            tweet_edit = re.sub(r'[^\\w\\s]','',tweet_text)\n",
    "            tokens = filter(tweet_edit)\n",
    "            t = TextBlob(tweet_text)\n",
    "            sentiment = t.sentiment.polarity\n",
    "            tweet_ds.append([tweet_text, tokens, sentiment])\n",
    "        else:\n",
    "            continue\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(tweet_ds, columns = {'tweet', 'tokens', 'sentiment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet', 'sentiment', 'tokens'], dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.rename(columns = {'tweet':'tweet','sentiment':'tokens','tokens':'sentiment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'Out',\n",
       " 'TextBlob',\n",
       " 'TweetTokenizer',\n",
       " '_',\n",
       " '_12',\n",
       " '__',\n",
       " '___',\n",
       " '__builtin__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_dh',\n",
       " '_i',\n",
       " '_i1',\n",
       " '_i10',\n",
       " '_i11',\n",
       " '_i12',\n",
       " '_i13',\n",
       " '_i14',\n",
       " '_i15',\n",
       " '_i16',\n",
       " '_i17',\n",
       " '_i18',\n",
       " '_i19',\n",
       " '_i2',\n",
       " '_i20',\n",
       " '_i21',\n",
       " '_i22',\n",
       " '_i3',\n",
       " '_i4',\n",
       " '_i5',\n",
       " '_i6',\n",
       " '_i7',\n",
       " '_i8',\n",
       " '_i9',\n",
       " '_ih',\n",
       " '_ii',\n",
       " '_iii',\n",
       " '_oh',\n",
       " '_sh',\n",
       " 'data',\n",
       " 'exit',\n",
       " 'filter',\n",
       " 'get_ipython',\n",
       " 'i',\n",
       " 'json',\n",
       " 'line',\n",
       " 'pd',\n",
       " 'plt',\n",
       " 'quit',\n",
       " 're',\n",
       " 'sentiment',\n",
       " 'stopWords',\n",
       " 'stopwords',\n",
       " 't',\n",
       " 'tokenizer',\n",
       " 'tokens',\n",
       " 'tweet',\n",
       " 'tweet_ds',\n",
       " 'tweet_edit',\n",
       " 'tweet_text',\n",
       " 'tweets_data',\n",
       " 'tweets_file']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting variables to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del tweets_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del tweet_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del stopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Loading dependencies for our classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs, multiprocessing, os, pprint, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaurav Ahlawat\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## !pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## data = pd.read_csv('dl_data_for_sentiment.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>320514</th>\n",
       "      <td>RT @NK_ESQ: So no FA interest in Wenger's comm...</td>\n",
       "      <td>[rt, nk_esq, so, no, fa, interest, in, wengers...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320515</th>\n",
       "      <td>@PaulAhdal @GazetteBoro @PTallentireGaz ever E...</td>\n",
       "      <td>[paulahdal, gazetteboro, ptallentiregaz, ever,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320516</th>\n",
       "      <td>\"Guns from the Vanguard arsenal are great. The...</td>\n",
       "      <td>[guns, from, the, vanguard, arsenal, are, grea...</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320517</th>\n",
       "      <td>#YIAYwar Captan América will see the city and ...</td>\n",
       "      <td>[yiaywar, captan, américa, will, see, the, cit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320518</th>\n",
       "      <td>Quite an interesting thing about Ashley Young....</td>\n",
       "      <td>[quite, an, interesting, thing, about, ashley,...</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    tweet  \\\n",
       "320514  RT @NK_ESQ: So no FA interest in Wenger's comm...   \n",
       "320515  @PaulAhdal @GazetteBoro @PTallentireGaz ever E...   \n",
       "320516  \"Guns from the Vanguard arsenal are great. The...   \n",
       "320517  #YIAYwar Captan América will see the city and ...   \n",
       "320518  Quite an interesting thing about Ashley Young....   \n",
       "\n",
       "                                                   tokens sentiment  \n",
       "320514  [rt, nk_esq, so, no, fa, interest, in, wengers...         0  \n",
       "320515  [paulahdal, gazetteboro, ptallentiregaz, ever,...         0  \n",
       "320516  [guns, from, the, vanguard, arsenal, are, grea...  0.483333  \n",
       "320517  [yiaywar, captan, américa, will, see, the, cit...         0  \n",
       "320518  [quite, an, interesting, thing, about, ashley,...       0.3  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt',\n",
       " 'johnryan',\n",
       " '89',\n",
       " 'oh',\n",
       " 'mané',\n",
       " 'mané',\n",
       " '3',\n",
       " 'points',\n",
       " 'important',\n",
       " 'with',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'big',\n",
       " 'boys',\n",
       " 'taking',\n",
       " 'points',\n",
       " 'off',\n",
       " 'each',\n",
       " 'other',\n",
       " 'tomorrow',\n",
       " 'lfc',\n",
       " 'whuliv',\n",
       " 'btsport']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.tokens, data.sentiment, test_size = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labelize_tweet(tweets, label_type):\n",
    "    labelized = []\n",
    "    for i, v in enumerate(tweets):\n",
    "        label = '%s_%s' %(label_type, i)\n",
    "        labelized.append(LabeledSentence(v,[label]))\n",
    "    return labelized\n",
    "\n",
    "X_train = labelize_tweet(X_train, 'train')\n",
    "X_test = labelize_tweet(X_test, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledSentence(words=['what', 'a', 'save', 'from', 'the', 'spurs', 'keeper'], tags=['train_0'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a pre-trained word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets2vec = w2v.Word2Vec.load(os.path.join(\"trained2\", \"tweets2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just testing whether it works or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93291073010698011"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets2vec.similarity('united', 'chelsea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer = lambda x:x, min_df = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix = vectorizer.fit_transform([x.words for x in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = dict(zip(vectorizer.get_feature_names(),vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 13252\n"
     ]
    }
   ],
   "source": [
    "print('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += tweets2vec[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating weighted averages of word vectors using Tf-IDf scores on word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "train_vecs_w2v = np.concatenate([build_word_vector(z, 200) for z in (map(lambda x: x.words, X_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([build_word_vector(z, 200) for z in (map(lambda x: x.words, X_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-1.4.0-cp36-cp36m-win_amd64.whl (28.3MB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\gaurav ahlawat\\anaconda3\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: numpy>=1.12.1 in c:\\users\\gaurav ahlawat\\anaconda3\\lib\\site-packages (from tensorflow)\n",
      "Collecting tensorflow-tensorboard<0.5.0,>=0.4.0rc1 (from tensorflow)\n",
      "  Downloading tensorflow_tensorboard-0.4.0rc3-py3-none-any.whl (1.7MB)\n",
      "Collecting enum34>=1.1.6 (from tensorflow)\n",
      "  Downloading enum34-1.1.6-py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\gaurav ahlawat\\anaconda3\\lib\\site-packages (from tensorflow)\n",
      "Collecting protobuf>=3.3.0 (from tensorflow)\n",
      "  Downloading protobuf-3.5.0.post1-py2.py3-none-any.whl (389kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in c:\\users\\gaurav ahlawat\\anaconda3\\lib\\site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow)\n",
      "Collecting markdown>=2.6.8 (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow)\n",
      "  Downloading Markdown-2.6.9.tar.gz (271kB)\n",
      "Collecting html5lib==0.9999999 (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow)\n",
      "  Downloading html5lib-0.9999999.tar.gz (889kB)\n",
      "Requirement already satisfied: bleach==1.5.0 in c:\\users\\gaurav ahlawat\\anaconda3\\lib\\site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gaurav ahlawat\\anaconda3\\lib\\site-packages\\setuptools-27.2.0-py3.6.egg (from protobuf>=3.3.0->tensorflow)\n",
      "Building wheels for collected packages: markdown, html5lib\n",
      "  Running setup.py bdist_wheel for markdown: started\n",
      "  Running setup.py bdist_wheel for markdown: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Gaurav Ahlawat\\AppData\\Local\\pip\\Cache\\wheels\\bf\\46\\10\\c93e17ae86ae3b3a919c7b39dad3b5ccf09aeb066419e5c1e5\n",
      "  Running setup.py bdist_wheel for html5lib: started\n",
      "  Running setup.py bdist_wheel for html5lib: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Gaurav Ahlawat\\AppData\\Local\\pip\\Cache\\wheels\\6f\\85\\6c\\56b8e1292c6214c4eb73b9dda50f53e8e977bf65989373c962\n",
      "Successfully built markdown html5lib\n",
      "Installing collected packages: markdown, html5lib, protobuf, tensorflow-tensorboard, enum34, tensorflow\n",
      "  Found existing installation: html5lib 0.999\n",
      "    Uninstalling html5lib-0.999:\n",
      "      Successfully uninstalled html5lib-0.999\n",
      "Successfully installed enum34-1.1.6 html5lib-0.9999999 markdown-2.6.9 protobuf-3.5.0.post1 tensorflow-1.4.0 tensorflow-tensorboard-0.4.0rc3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    DEPRECATION: Uninstalling a distutils installed project (html5lib) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This simple two layer model is showing some really bad accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The reason might be the low number of tweets, which are around 300k, out of which I suspect some have sentiment equal to 0, let's check it out, but before that, lets try and see if increasing the complexity of the model by adding new layers has any substantial impact on the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 12s - loss: 0.2342 - acc: 0.4449\n",
      "Epoch 2/10\n",
      " - 11s - loss: 0.1755 - acc: 0.4453\n",
      "Epoch 3/10\n",
      " - 10s - loss: 0.1450 - acc: 0.4456\n",
      "Epoch 4/10\n",
      " - 11s - loss: 0.1245 - acc: 0.4462\n",
      "Epoch 5/10\n",
      " - 11s - loss: 0.1104 - acc: 0.4461\n",
      "Epoch 6/10\n",
      " - 10s - loss: 0.0994 - acc: 0.4464\n",
      "Epoch 7/10\n",
      " - 11s - loss: 0.0903 - acc: 0.4463\n",
      "Epoch 8/10\n",
      " - 11s - loss: 0.0825 - acc: 0.4463\n",
      "Epoch 9/10\n",
      " - 11s - loss: 0.0760 - acc: 0.4467\n",
      "Epoch 10/10\n",
      " - 12s - loss: 0.0702 - acc: 0.4470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c18d948a20>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.add(Dense(32, activation='relu', input_dim=200))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vecs_w2v, y_train, epochs=10, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No effect at all!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 11s - loss: 0.0663 - acc: 0.4471\n",
      "Epoch 2/10\n",
      " - 11s - loss: 0.0607 - acc: 0.4470\n",
      "Epoch 3/10\n",
      " - 10s - loss: 0.0575 - acc: 0.4474\n",
      "Epoch 4/10\n",
      " - 11s - loss: 0.0532 - acc: 0.4473\n",
      "Epoch 5/10\n",
      " - 11s - loss: 0.0494 - acc: 0.4475\n",
      "Epoch 6/10\n",
      " - 11s - loss: 0.0455 - acc: 0.4477\n",
      "Epoch 7/10\n",
      " - 11s - loss: 0.0414 - acc: 0.4478\n",
      "Epoch 8/10\n",
      " - 11s - loss: 0.0371 - acc: 0.4478\n",
      "Epoch 9/10\n",
      " - 10s - loss: 0.0339 - acc: 0.4477\n",
      "Epoch 10/10\n",
      " - 11s - loss: 0.0323 - acc: 0.4482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c18d97b588>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Dense(32, activation='relu', input_dim=200))\n",
    "model1.add(Dense(32, activation='relu', input_dim=200))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vecs_w2v, y_train, epochs=10, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of                                                     tweet  \\\n",
       "0       RT @johnryan89: Oh Mané, Mané. 3 points import...   \n",
       "1       Please RT!! #liverpool #liverpoolFC #LFC   \"Mo...   \n",
       "2       RT @JARiiseOfficial: Watching the boys @LFC he...   \n",
       "3       @JackBMontgomery Scum. Here are Liverpool poli...   \n",
       "4       RT @thisisanfield: \"If it was me and I was in ...   \n",
       "5       RT @footballpools: Players of the season so fa...   \n",
       "6       Foamin I’m going to Liverpool for my 21st prac...   \n",
       "7       Looking forward to getting home and watching #...   \n",
       "8       RT @TheSportsman: 😏 Goals this season…\\n\\n- Ox...   \n",
       "9       @shibani_mufc The one against Liverpool. That ...   \n",
       "10      RT @GoalcomNigeria: Mohamed Salah likes new ro...   \n",
       "11      @TPEassist every time I travel with TPE Liverp...   \n",
       "12      RT @BIG_RIVER_CITY: .@DodgyUK\\n#AcousticTour\\n...   \n",
       "13      RT @Jaack: Liverpool putting on Lovren after b...   \n",
       "14      Betting Preview: West Ham United v Liverpool #...   \n",
       "15      RT @LFC: Good work, boys! 🔴 https://t.co/xjIji...   \n",
       "16      KarloMdz: UTVNews: Family pay tribute to Co An...   \n",
       "17      Stoke's Peter Crouch fails to rise to occasion...   \n",
       "18      RT @MirrorFootball: Why Liverpool forward Robe...   \n",
       "19      Mohamed Salah likes new role - Liverpool's Klo...   \n",
       "20      @Jesse_Rai @LFC @Alex_OxChambo Happy for him b...   \n",
       "21      RT @bet365: Best players of the season so far:...   \n",
       "22      RT @thisisanfield: NEW: Steven Gerrard urges E...   \n",
       "23      RT @BIG_RIVER_CITY: #SOUNDSUNDAY @castofficial...   \n",
       "24      [Goal] Mohamed Salah likes new role - Liverpoo...   \n",
       "25      RT @LFC: Good work, boys! 🔴 https://t.co/xjIji...   \n",
       "26      @83ynwa @AsimRasib @LFC @SMignolet Highest pai...   \n",
       "27      RT @westhamfantv: 'Do we wanna sack Bilic if w...   \n",
       "28      RT @BIG_RIVER_CITY: .@DodgyUK\\n#AcousticTour\\n...   \n",
       "29      RT @LFC: OXLADE-CHAMBERLAIN! \\n\\nWHAT A RESPON...   \n",
       "...                                                   ...   \n",
       "320489      @Joemcfc_ Sane has 3 according to PL website?   \n",
       "320490  RT @ManUtd: Jose Mourinho has told #MUTV he do...   \n",
       "320491  RT @OfficialCheIs: Conte gets charged for gett...   \n",
       "320492  RT @QueenWillRock: \"...a dazzling spectacle, v...   \n",
       "320493  RT @ManUtd_HQ: Games with 4+ goals scored this...   \n",
       "320494  RT @ManUtd: With over two-thirds of the vote, ...   \n",
       "320495  RT @JamesPearceEcho: Klopp on Hughes' complain...   \n",
       "320496  RT @JamesPearceEcho: Klopp on Hughes' complain...   \n",
       "320497  RT @MufcDevilUpdate: Andreas Pereira training ...   \n",
       "320498  Wednesday night - Pep Guardiola explains incid...   \n",
       "320499  RT @TotallyMUFC: When Arsenal has to welcome R...   \n",
       "320500  @vvijnaldum @TheLampardWay No EPL title, only ...   \n",
       "320501  If Herrera starts we’re getting banged 5-0 htt...   \n",
       "320502  @PatrickTimmons1 Ramsey has been key to arsena...   \n",
       "320503  RT @ChelsLad66: So in a week where Lukaku kick...   \n",
       "320504  RT @FootbalIStuff: \"Gattuso, what do you think...   \n",
       "320505   @andoculture Im comparing PL midfields generally   \n",
       "320506  Cue the bullshit story which the FA will belie...   \n",
       "320507  RT @Footy_Faithful: ‼️NEW POD‼️ Premier League...   \n",
       "320508  RT @MourinhoMindset: Jose Mourinho takes one s...   \n",
       "320509  What a wonderful lunatic Guardiola is. https:/...   \n",
       "320510  RT @CharlieRidgewel: Barber niced me again, I’...   \n",
       "320511  RT @dlrbrts: Ever wondered what Manchester Cit...   \n",
       "320512  RT @UnitedStandMUFC: The exact moment Roy Kean...   \n",
       "320513  RT @SemperFiUtd: \"Too far for Ronaldo to think...   \n",
       "320514  RT @NK_ESQ: So no FA interest in Wenger's comm...   \n",
       "320515  @PaulAhdal @GazetteBoro @PTallentireGaz ever E...   \n",
       "320516  \"Guns from the Vanguard arsenal are great. The...   \n",
       "320517  #YIAYwar Captan América will see the city and ...   \n",
       "320518  Quite an interesting thing about Ashley Young....   \n",
       "\n",
       "                                                   tokens  sentiment  \n",
       "0       [rt, johnryan, 89, oh, mané, mané, 3, points, ...  0.0916667  \n",
       "1       [please, rt, liverpool, liverpoolfc, lfc, mo, ...       -0.7  \n",
       "2       [rt, jariiseofficial, watching, the, boys, lfc...       -0.4  \n",
       "3       [jackbmontgomery, scum, here, are, liverpool, ...       -0.3  \n",
       "4       [rt, thisisanfield, if, it, was, me, and, i, w...          0  \n",
       "5       [rt, footballpools, players, of, the, season, ...        0.1  \n",
       "6       [foamin, im, going, to, liverpool, for, my, 21...          0  \n",
       "7       [looking, forward, to, getting, home, and, wat...          0  \n",
       "8       [rt, thesportsman, goals, this, season, oxlade...          0  \n",
       "9       [shibani_mufc, the, one, against, liverpool, t...          0  \n",
       "10      [rt, goalcomnigeria, mohamed, salah, likes, ne...   0.136364  \n",
       "11      [tpeassist, every, time, i, travel, with, tpe,...          0  \n",
       "12      [rt, big_river_city, dodgyuk, acoustictour, ne...          0  \n",
       "13      [rt, jaack, liverpool, putting, on, lovren, af...   0.136364  \n",
       "14      [betting, preview, west, ham, united, v, liver...          0  \n",
       "15                        [rt, lfc, good, work, boys, 8s]      0.875  \n",
       "16      [karlomdz, utvnews, family, pay, tribute, to, ...       -0.2  \n",
       "17      [stokes, peter, crouch, fails, to, rise, to, o...       -0.5  \n",
       "18      [rt, mirrorfootball, why, liverpool, forward, ...      -0.25  \n",
       "19      [mohamed, salah, likes, new, role, liverpools,...   0.136364  \n",
       "20      [jesse_rai, lfc, alex_oxchambo, happy, for, hi...        0.8  \n",
       "21      [rt, bet, 365, best, players, of, the, season,...       0.55  \n",
       "22      [rt, thisisanfield, new, steven, gerrard, urge...   0.136364  \n",
       "23      [rt, big_river_city, soundsunday, castofficial...          0  \n",
       "24      [goal, mohamed, salah, likes, new, role, liver...   0.136364  \n",
       "25                        [rt, lfc, good, work, boys, 8s]      0.875  \n",
       "26      [83ynwa, asimrasib, lfc, smignolet, highest, p...        0.2  \n",
       "27      [rt, westhamfantv, do, we, wanna, sack, bilic,...          0  \n",
       "28      [rt, big_river_city, dodgyuk, acoustictour, ne...          0  \n",
       "29      [rt, lfc, oxladechamberlain, what, a, response...          0  \n",
       "...                                                   ...        ...  \n",
       "320489  [joemcfc, _, sane, has, 3, according, to, pl, ...          0  \n",
       "320490  [rt, manutd, jose, mourinho, has, told, mutv, ...          0  \n",
       "320491  [rt, officialcheis, conte, gets, charged, for,...       -0.5  \n",
       "320492  [rt, queenwillrock, a, dazzling, spectacle, vi...      0.625  \n",
       "320493  [rt, manutd_hq, games, with, 4, goals, scored,...        0.5  \n",
       "320494  [rt, manutd, with, over, twothirds, of, the, v...          0  \n",
       "320495  [rt, jamespearceecho, klopp, on, hughes, compl...      -0.25  \n",
       "320496  [rt, jamespearceecho, klopp, on, hughes, compl...      -0.25  \n",
       "320497  [rt, mufcdevilupdate, andreas, pereira, traini...          0  \n",
       "320498  [wednesday, night, pep, guardiola, explains, i...          0  \n",
       "320499  [rt, totallymufc, when, arsenal, has, to, welc...        0.4  \n",
       "320500  [vvijnaldum, thelampardway, no, epl, title, on...          0  \n",
       "320501   [if, herrera, starts, were, getting, banged, 50]          0  \n",
       "320502  [patricktimmons, 1, ramsey, has, been, key, to...   0.153125  \n",
       "320503  [rt, chelslad, 66, so, in, a, week, where, luk...          0  \n",
       "320504  [rt, footbalistuff, gattuso, what, do, you, th...          0  \n",
       "320505  [andoculture, im, comparing, pl, midfields, ge...       0.05  \n",
       "320506  [cue, the, bullshit, story, which, the, fa, wi...          0  \n",
       "320507  [rt, footy_faithful, new, pod, premier, league...          1  \n",
       "320508  [rt, mourinhomindset, jose, mourinho, takes, o...       -0.1  \n",
       "320509  [what, a, wonderful, lunatic, guardiola, is, 6...          1  \n",
       "320510  [rt, charlieridgewel, barber, niced, me, again...          0  \n",
       "320511  [rt, dlrbrts, ever, wondered, what, manchester...          0  \n",
       "320512  [rt, unitedstandmufc, the, exact, moment, roy,...       0.25  \n",
       "320513  [rt, semperfiutd, too, far, for, ronaldo, to, ...      0.125  \n",
       "320514  [rt, nk_esq, so, no, fa, interest, in, wengers...          0  \n",
       "320515  [paulahdal, gazetteboro, ptallentiregaz, ever,...          0  \n",
       "320516  [guns, from, the, vanguard, arsenal, are, grea...   0.483333  \n",
       "320517  [yiaywar, captan, américa, will, see, the, cit...          0  \n",
       "320518  [quite, an, interesting, thing, about, ashley,...        0.3  \n",
       "\n",
       "[320519 rows x 3 columns]>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

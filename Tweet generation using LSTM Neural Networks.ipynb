{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this we are going to use RNNs(Recurrent Neural Networks) and specifically LSTM, i.e, Long Short Term Memory cells for generating character by character text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the input text, we are going to train all the tweets at once, i.e., normally people feed tweets as separate entities for the model to train on, but my results when I tried to do so were very bad, mostly a single character was predicted each and every time, so I am still figuring out if it was due to some error I made or something else. So for now lets feed tweets together, where one tweet follows other as  a sentence follows another in a big text document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = \"liver_for_generation.txt\"\n",
    "raw_text = open(filename, encoding=\"utf8\").read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408124"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating mapping from characters to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the reverse mapping from integer to character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Reason for the poor performance of the text generated using tweets as an input for model to train on is due to this huge vocabulary. We can try and reduce the vocabulary, but then what we would get out of it wouldn't necessarily be a tweet but a simple text of which you cannot make sense of because we might have removed some emoji's or some exclamation mark which might be important for understanding what message a person is trying to convey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total characters:  408124\n",
      "total vocab:  263\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"total characters: \", n_chars)\n",
    "print(\"total vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the different patterns\n",
    "These patterns are what we feed into the RNN network. The network then trains and tries to learn how all these patterns can be used to generate next character by creating a probabilistic model. Like a particular pattern occurs a lot and the character occuring after it always, let's say 'a', then the model will almost be sure to output 'a' when it sees this pattern.\n",
    "But in most cases, there wouldn't be such patterns which lead to a sure shot character, so all the characters in the vocab get probabilities for different patterns and these probabilities are used by the model to get the most probable character and outputs it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_length = 140\n",
    "data_input_X = []\n",
    "data_out_Y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total patterns:  407984\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i+seq_length]\n",
    "    seq_out = raw_text[i+ seq_length]\n",
    "    data_input_X.append([char_to_int[char] for char in seq_in])\n",
    "    data_out_Y.append([char_to_int[seq_out]])\n",
    "n_patterns = len(data_input_X)\n",
    "print(\"total patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the input data so as to make it a shape what a sequential model can intake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can say till now, all the data_input_X were columns, now they have been reshaped in proper rows*columns dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.reshape(data_input_X, (n_patterns, seq_length, 1))\n",
    "X = X/ float(n_vocab)\n",
    "y = np_utils.to_categorical(data_outY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "The first layer is an LSTM network layer, followed by a dropout layer and a final dense layer for making the decisions using Softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape = (X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the weights of the model for which the loss is less as compared to the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"weights-improvement-{epoch:02d}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only training for 3 Epochs, as it takes 2 hours for each one to be done with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "407936/407984 [============================>.] - ETA: 0s - loss: 3.2967Epoch 00001: loss improved from inf to 3.29666, saving model to weights-improvement-01-3.2967.hdf5\n",
      "407984/407984 [==============================] - 5075s 12ms/step - loss: 3.2967\n",
      "Epoch 2/3\n",
      "407936/407984 [============================>.] - ETA: 0s - loss: 2.8999Epoch 00002: loss improved from 3.29666 to 2.89986, saving model to weights-improvement-02-2.8999.hdf5\n",
      "407984/407984 [==============================] - 5030s 12ms/step - loss: 2.8999\n",
      "Epoch 3/3\n",
      "407936/407984 [============================>.] - ETA: 0s - loss: 2.7197Epoch 00003: loss improved from 2.89986 to 2.71975, saving model to weights-improvement-03-2.7197.hdf5\n",
      "407984/407984 [==============================] - 4437s 11ms/step - loss: 2.7197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f60b44d8d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=3, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"weights-improvement-03.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating tweets using a random seed as a start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = data_iput_X[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\"  going to be cancelleâ€¦ football365: mails: time for flying liverpool to drop emre can https://t.co/dsoa47qs6cbig wins in midweek for city an \"\n",
      " coee ao laa  aac cite po lee  aac cite po lee  aac lite po  lice lo tee he lee  aac  ht caa  hic hi\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    pred = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(pred)\n",
    "    res = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(res)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" or liverpool instead of arsenalâ€¦ https://t.co/ymp7bfuxpsrt @lfc: three points to take back home! ðŸ”´ https://t.co/iuxjrty3dagreat choice @bbcr \"\n",
      " la  hiverpool hat rit  afc cite po lee  aac cite po lee  aac cite po lee  aac cite po lee  aac cite po lee  aac cite po lee  aac cite po le\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = data_iput_X[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    pred = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(pred)\n",
    "    res = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(res)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" irgin trains1f14 12:05 euston to liverpool lime streetrt @josh_pne: these are women who just want to be able to play football. we should be  \"\n",
      "liee  hac hite pot  afte to lee  aat cit spe lene  l coals \n",
      "aa lone aa goae no lee  aac cite pot re \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = data_iput_X[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    pred = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(pred)\n",
    "    res = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(res)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\"  kick everton, they will. those same people who whitewash lfc time after time. wâ€¦ https://t.co/bcwjdwbf2xrt @lfc: 8âƒ£ days\n",
      "ðŸ”Ÿ goals scored\n",
      "1âƒ£  \"\n",
      "goals \n",
      "1\n",
      "\n",
      "ðŸŽ¯ gaals \n",
      "-\n",
      "\n",
      "aaseaad mttps://t.co/elnhh0dkdlrt @lfc: 8âƒ£ gaals \n",
      "1\n",
      "\n",
      "aare aa moal  lag cit  -t\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = data_iput_X[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    pred = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(pred)\n",
    "    res = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(res)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" : watching the boys @lfc here in indonesia now before our masters game tomorrow against arsenal. give us the 3 points plâ€¦@jackbmontgomery sc \"\n",
      "te se tee  aac lite po  hice lo tee he loe sene  ha  hiverpool 4aa  oiverpool 4-0 aisenpl laa cite 9\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = data_iput_X[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    pred = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(pred)\n",
    "    res = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(res)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
